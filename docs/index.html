<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PyTorch C++ Cheatsheet</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=72bcf2f2" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=5070d0a7" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/logo.png" alt="Logo"/>
    
  </a>
</p>









  <div>
    <h3><a href="#">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">PyTorch C++ Cheatsheet</a><ul>
<li><a class="reference internal" href="#tensor-creation">Tensor creation</a></li>
<li><a class="reference internal" href="#tensor-shape">Tensor shape</a></li>
<li><a class="reference internal" href="#tensor-data-type">Tensor data type</a></li>
<li><a class="reference internal" href="#tensor-efficient-accessors">Tensor (efficient) accessors</a></li>
<li><a class="reference internal" href="#tensor-inefficient-indexing">Tensor (inefficient) indexing</a></li>
<li><a class="reference internal" href="#tensor-memory-and-device">Tensor memory and device</a></li>
<li><a class="reference internal" href="#dispatching">Dispatching</a></li>
<li><a class="reference internal" href="#tensor-creation-from-raw-data">Tensor creation from raw data</a></li>
</ul>
</li>
</ul>

  </div>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="pytorch-c-cheatsheet">
<h1>PyTorch C++ Cheatsheet<a class="headerlink" href="#pytorch-c-cheatsheet" title="Link to this heading">§</a></h1>
<p>A unique C++ file showing how to use the <a class="reference external" href="https://pytorch.org/cppdocs">PyTorch C++ API</a> : <a class="reference external" href="https://github.com/ThibaultLejemble/PyTorch-CPP-Cheatsheet/blob/main/pytorch_cpp_cheatsheet.cpp">pytorch_cpp_cheatsheet.cpp</a>.</p>
<p>To build and run the C++ executable, checkout the GitHub repository <a class="reference external" href="https://github.com/ThibaultLejemble/PyTorch-CPP-Cheatsheet">here</a>.</p>
<section id="tensor-creation">
<h2>Tensor creation<a class="headerlink" href="#tensor-creation" title="Link to this heading">§</a></h2>
<div class="line-block">
<div class="line">See also <a class="reference external" href="https://pytorch.org/cppdocs/notes/tensor_creation.html">tensor_creation.html</a></div>
<div class="line">Factory arguments order = <cite>(function-specific, sizes, options)</cite> (except for <cite>torch::full</cite>)</div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">opts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">()</span>
<span class="w">    </span><span class="p">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)</span><span class="w">     </span><span class="c1">// kUInt8, kInt8, kInt16, kInt32, kInt64, kFloat32, or kFloat64</span>
<span class="w">    </span><span class="p">.</span><span class="n">layout</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kStrided</span><span class="p">)</span><span class="w">    </span><span class="c1">// kStrided, or kSparse</span>
<span class="w">    </span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">)</span><span class="w">        </span><span class="c1">// kCPU, or kCUDA</span>
<span class="w">    </span><span class="p">.</span><span class="n">requires_grad</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span><span class="w">      </span><span class="c1">// true, or false</span>

<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// 0</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">(</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// 1</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// uninitialized</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">full</span><span class="p">(</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="mf">3.14f</span><span class="p">,</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">   </span><span class="c1">// single value (warning: args order)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">eye</span><span class="p">(</span><span class="w">  </span><span class="mi">5</span><span class="p">,</span><span class="w">     </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// identity matrix (square)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">eye</span><span class="p">(</span><span class="w">  </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">  </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// identity matrix (rectangular)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">(</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// uniform distribution on [0, 1)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">          </span><span class="c1">// center-normalized normal distribution</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w">    </span><span class="c1">// random integers (low=0)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">opts</span><span class="p">);</span><span class="w"> </span><span class="c1">// random integers in (low,high)</span>

<span class="c1">// multiple chained short options</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat64</span><span class="p">).</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">));</span>

<span class="c1">// only one option (directly given as parameter)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat64</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="tensor-shape">
<h2>Tensor shape<a class="headerlink" href="#tensor-shape" title="Link to this heading">§</a></h2>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">});</span>

<span class="c1">// dimension</span>
<span class="kt">int</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">dim</span><span class="p">();</span>
<span class="n">assert</span><span class="p">(</span><span class="n">dim</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="c1">// sizes</span>
<span class="n">torch</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sizes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">sizes</span><span class="p">();</span>
<span class="n">assert</span><span class="p">(</span><span class="n">sizes</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">}));</span><span class="w"> </span><span class="c1">// comparable to std::vector&lt;int64_t&gt;</span>

<span class="c1">// single size</span>
<span class="kt">int</span><span class="w"> </span><span class="n">size1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">size1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="tensor-data-type">
<h2>Tensor data type<a class="headerlink" href="#tensor-data-type" title="Link to this heading">§</a></h2>
<div class="line-block">
<div class="line">Types are constexpr values of enum class type <cite>torch::ScalarType</cite></div>
<div class="line">See the enum class type here: <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69301fb10eb3f7fd49af5c681a2e386af115baba/c10/core/ScalarType.h#L151C12-L151C22">ScalarType.h</a></div>
<div class="line">See the different types here: <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69301fb10eb3f7fd49af5c681a2e386af115baba/torch/csrc/api/include/torch/types.h">types.h</a></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">type_float32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">;</span><span class="w"> </span><span class="c1">// or torch::kF32</span>

<span class="c1">// map torch to c++ type</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="o">&gt;::</span><span class="n">type</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="p">);</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat64</span><span class="o">&gt;::</span><span class="n">type</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="p">);</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">kInt32</span><span class="o">&gt;::</span><span class="n">type</span><span class="p">,</span><span class="w">   </span><span class="kt">int</span><span class="o">&gt;</span><span class="p">);</span>

<span class="c1">// map c++ to torch type (Cpp not CPP!)</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">CppTypeToScalarType</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;::</span><span class="n">value</span><span class="w">  </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">);</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">CppTypeToScalarType</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;::</span><span class="n">value</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat64</span><span class="p">);</span>
<span class="k">static_assert</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">CppTypeToScalarType</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;::</span><span class="n">value</span><span class="w">    </span><span class="o">==</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kInt32</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span><span class="w"> </span><span class="c1">// print &#39;Float&#39;</span>
<span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">elementSize</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">);</span><span class="w">   </span><span class="c1">// dynamic equivalent of sizeof(float)</span>
<span class="n">assert</span><span class="p">(</span><span class="n">bytes</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span><span class="w"> </span><span class="c1">// 4 bytes = 32 bits</span>

<span class="n">assert</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">isIntegralType</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="cm">/*includeBool=*/</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">isFloatingType</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="tensor-efficient-accessors">
<h2>Tensor (efficient) accessors<a class="headerlink" href="#tensor-efficient-accessors" title="Link to this heading">§</a></h2>
<div class="line-block">
<div class="line">See also <a class="reference external" href="https://pytorch.org/cppdocs/notes/tensor_basics.html#efficient-access-to-tensor-elements">tensor_basics.html</a></div>
<div class="line">Type and dimension must be known at compile-time</div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">});</span>

<span class="c1">// cpu</span>
<span class="k">auto</span><span class="w"> </span><span class="n">accessor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">accessor</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="c1">// dim = 3</span>
<span class="n">accessor</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">3.14f</span><span class="p">;</span>
<span class="kt">float</span><span class="o">&amp;</span><span class="w"> </span><span class="n">ref</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">accessor</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">];</span>
</pre></div>
</div>
</section>
<section id="tensor-inefficient-indexing">
<h2>Tensor (inefficient) indexing<a class="headerlink" href="#tensor-inefficient-indexing" title="Link to this heading">§</a></h2>
<div class="line-block">
<div class="line">See also <a class="reference external" href="https://pytorch.org/cppdocs/notes/tensor_indexing.html">tensor_indexing.html</a></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">});</span>
<span class="n">x</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">})</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">3.14f</span><span class="p">;</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">index</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">});</span>
<span class="n">assert</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"> </span><span class="c1">// 0-dim tensor = juste one value</span>
<span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="tensor-memory-and-device">
<h2>Tensor memory and device<a class="headerlink" href="#tensor-memory-and-device" title="Link to this heading">§</a></h2>
<div class="line-block">
<div class="line">Row-major order by default</div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">});</span>

<span class="c1">// memory ptr</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">raw_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">();</span>
<span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">typed_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

<span class="c1">// device</span>
<span class="n">assert</span><span class="p">(</span><span class="w">    </span><span class="n">x</span><span class="p">.</span><span class="n">is_cpu</span><span class="p">());</span>
<span class="n">assert</span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">is_cuda</span><span class="p">());</span>

<span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">();</span>
<span class="n">assert</span><span class="p">(</span><span class="w">    </span><span class="n">device</span><span class="p">.</span><span class="n">is_cpu</span><span class="p">());</span>
<span class="n">assert</span><span class="p">(</span><span class="k">not</span><span class="w"> </span><span class="n">device</span><span class="p">.</span><span class="n">is_cuda</span><span class="p">());</span>

<span class="c1">// memory layout</span>
<span class="n">assert</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">is_contiguous</span><span class="p">());</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Layout</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">layout</span><span class="p">();</span>
<span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">stride0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">stride1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="kt">long</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">stride2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">stride0</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">4</span><span class="o">*</span><span class="mi">5</span><span class="p">);</span><span class="w"> </span><span class="c1">// x[0,i,j] and x[1,i,j] are 10 values apart</span>
<span class="n">assert</span><span class="p">(</span><span class="n">stride1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
<span class="n">assert</span><span class="p">(</span><span class="n">stride2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="dispatching">
<h2>Dispatching<a class="headerlink" href="#dispatching" title="Link to this heading">§</a></h2>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">});</span>
<span class="c1">// AT_DISPATCH_ALL_TYPES(x.scalar_type(), &quot;unique_name&quot;, [&amp;] {</span>
<span class="c1">//     scalar_t val;</span>
<span class="c1">// });</span>
</pre></div>
</div>
</section>
<section id="tensor-creation-from-raw-data">
<h2>Tensor creation from raw data<a class="headerlink" href="#tensor-creation-from-raw-data" title="Link to this heading">§</a></h2>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">raw_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span>
<span class="w">    </span><span class="n">raw_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="c1">// data = [0,  1,  2,  3,</span>
<span class="c1">//         4,  5,  6,  7,</span>
<span class="c1">//         8,  9, 10, 11]</span>
<span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">deleter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="kt">void</span><span class="o">*</span><span class="p">){</span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;delete!&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;};</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">from_blob</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">},</span><span class="w"> </span><span class="n">deleter</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kInt32</span><span class="p">));</span>
<span class="c1">// end of scope prints &quot;delete!&quot;</span>
</pre></div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2025, Thibault Lejemble.
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>